{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "37a92655-7158-4c99-8ace-0632004a59d0",
   "metadata": {},
   "source": [
    "# RNN Input Data Analysis and Diagonise "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d2a98ced-7bda-4d11-bfa7-41c3013a1466",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b71424df-10b2-4663-9646-401b88cd993f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The directory /net2/sandbox/willetspeech_decoder/rnn_cls/input/randTVT_TX1-SP exists.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "derbaseDir =  ... \n",
    "sessionName = 't12.2022.05.05' # 't12.2022.05.03_fiftyWordSet' # 't12.2022.04.28'\n",
    "\n",
    "\n",
    "import os\n",
    "\n",
    "if os.path.isdir(derbaseDir):\n",
    "    print(f\"The directory {derbaseDir} exists.\")\n",
    "else:\n",
    "    print(f\"The directory {derbaseDir} does not exist.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8b0c7f0-73ea-4b50-b6c1-ee4fa1b6c7bb",
   "metadata": {},
   "source": [
    "# Dataset Schema "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f13882ec-7164-4d34-b86a-21ff2440ad87",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-13 00:44:11.346704: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: \n",
      "2024-09-13 00:44:11.346798: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "/home/jih201/.local/lib/python3.9/site-packages/pandas/core/arrays/masked.py:60: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.5' currently installed).\n",
      "  from pandas.core import (\n"
     ]
    }
   ],
   "source": [
    "import pathlib\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "isTraining = False \n",
    "\n",
    "def _loadDataset(fileDir):\n",
    "    files = sorted([str(x) for x in pathlib.Path(fileDir).glob(\"*.tfrecord\")])\n",
    "    print(files)\n",
    "    if isTraining:\n",
    "        random.shuffle(files)\n",
    "    dataset = tf.data.TFRecordDataset(files)\n",
    "    return dataset\n",
    "\n",
    "nInputFeatures = 256\n",
    "chanIndices = np.array(range(nInputFeatures))\n",
    "maxSeqElements = 100 # 100 for phoneme \n",
    "datasetFeatures = {\n",
    "    \"inputFeatures\": tf.io.FixedLenSequenceFeature([nInputFeatures], tf.float32, allow_missing=True),\n",
    "    #\"classLabelsOneHot\": tf.io.FixedLenSequenceFeature([self.nClasses+1], tf.float32, allow_missing=True),\n",
    "    \"newClassSignal\": tf.io.FixedLenSequenceFeature([], tf.float32, allow_missing=True),\n",
    "    \"ceMask\": tf.io.FixedLenSequenceFeature([], tf.float32, allow_missing=True),\n",
    "    \"seqClassIDs\": tf.io.FixedLenFeature((maxSeqElements), tf.int64),\n",
    "    \"nTimeSteps\": tf.io.FixedLenFeature((), tf.int64),\n",
    "    \"nSeqElements\": tf.io.FixedLenFeature((), tf.int64),\n",
    "    \"transcription\": tf.io.FixedLenFeature((maxSeqElements), tf.int64)\n",
    "}\n",
    "\n",
    "def parseDatasetFunctionSimple(exampleProto):\n",
    "    dat = tf.io.parse_single_example(exampleProto, datasetFeatures)\n",
    "    if chanIndices is not None:\n",
    "        newDat = {}\n",
    "        newDat['seqClassIDs'] = dat['seqClassIDs']\n",
    "        newDat['nSeqElements'] = dat['nSeqElements']\n",
    "        newDat['transcriptions'] = dat['transcription']\n",
    "        newDat['nTimeSteps'] = dat['nTimeSteps']\n",
    "        newDat['newClassSignal'] = dat['newClassSignal']\n",
    "        newDat['ceMask'] = dat['ceMask']\n",
    "        print(dat['inputFeatures'])\n",
    "        selectChans = tf.gather(dat['inputFeatures'], tf.constant(chanIndices),axis=-1)\n",
    "        paddings = [[0, 0], [0, 256-tf.shape(selectChans)[-1]]]\n",
    "        newDat['inputFeatures'] = tf.pad(selectChans, paddings, 'CONSTANT',constant_values=0)\n",
    "        print(tf.shape(newDat['inputFeatures']))\n",
    "        return newDat\n",
    "    else:\n",
    "        return dat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e8d32853-6268-40f8-8070-814a03b2a367",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_PHONE_DEF(): \n",
    "    PHONE_DEF = [\n",
    "        'N/A', 'AA', 'AE', 'AH', 'AO', 'AW',\n",
    "        'AY', 'B',  'CH', 'D', 'DH',\n",
    "        'EH', 'ER', 'EY', 'F', 'G',\n",
    "        'HH', 'IH', 'IY', 'JH', 'K',\n",
    "        'L', 'M', 'N', 'NG', 'OW',\n",
    "        'OY', 'P', 'R', 'S', 'SH',\n",
    "        'T', 'TH', 'UH', 'UW', 'V',\n",
    "        'W', 'Y', 'Z', 'ZH', 'SIL'\n",
    "    ]\n",
    "    return PHONE_DEF\n",
    "\n",
    "PHONE_DEF = get_PHONE_DEF()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0dd4159e-2cc3-4134-8a17-9a28f6e0b0bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming 0 is the padding value\n",
    "def remove_padding(tensor, padding_value=0):\n",
    "    mask = tf.not_equal(tensor, padding_value)  # Create a mask for non-padding values\n",
    "    cleaned_tensor = tf.boolean_mask(tensor, mask)  # Remove padding based on mask\n",
    "    return cleaned_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ca4df7a-38e7-49b6-b3c8-59eb4a03df04",
   "metadata": {},
   "source": [
    "# Load Dataset \n",
    "- seqClassIDs -> trueSeqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "498334e0-5c4a-4795-8360-188e98b71e71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/net2/sandbox/willetspeech_decoder/rnn_cls/input/randTVT_TX1-SP/t12.2022.05.05/train/chunk_0.tfrecord']\n",
      "Tensor(\"ParseSingleExample/ParseExample/ParseExampleV2:1\", shape=(None, 256), dtype=float32)\n",
      "Tensor(\"Shape_1:0\", shape=(2,), dtype=int32)\n",
      "['/net2/sandbox/willetspeech_decoder/rnn_cls/input/randTVT_TX1-SP/t12.2022.05.05/val/chunk_0.tfrecord']\n",
      "Tensor(\"ParseSingleExample/ParseExample/ParseExampleV2:1\", shape=(None, 256), dtype=float32)\n",
      "Tensor(\"Shape_1:0\", shape=(2,), dtype=int32)\n",
      "['/net2/sandbox/willetspeech_decoder/rnn_cls/input/randTVT_TX1-SP/t12.2022.05.05/test/chunk_0.tfrecord']\n",
      "Tensor(\"ParseSingleExample/ParseExample/ParseExampleV2:1\", shape=(None, 256), dtype=float32)\n",
      "Tensor(\"Shape_1:0\", shape=(2,), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "formated_data = {} \n",
    "\n",
    "for part in ['train', 'test']: \n",
    "    tfRecordFolder = derbaseDir+'/'+sessionName+'/'+part+'/'\n",
    "\n",
    "    formated_data[part] = {} \n",
    "    for fn in ['inputFeatures', 'transcriptions', 'trueText', 'trueSeqs', 'seqClassIDs', 'ClassIDs', 'seqStartIdx', 'trueSeqLengths']: \n",
    "        formated_data[part][fn] = [] \n",
    "    formated_data[part]['nTrials'] = 0\n",
    "    \n",
    "    parsed_dataset = _loadDataset(tfRecordFolder).map(parseDatasetFunctionSimple, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    formated_data[part]['seqStartIdx'].append(0) \n",
    "    for parsed_record in parsed_dataset:\n",
    "        formated_data[part]['inputFeatures'].append(parsed_record['inputFeatures'])\n",
    "        tmp = remove_padding(parsed_record['transcriptions'])\n",
    "        formated_data[part]['transcriptions'].append(tmp)\n",
    "        formated_data[part]['trueText'].append(''.join([chr(i) for i in tmp]))\n",
    "        tmp = remove_padding(parsed_record['seqClassIDs'])\n",
    "        formated_data[part]['seqClassIDs'].append(tmp)\n",
    "        formated_data[part]['trueSeqs'].append(parsed_record['seqClassIDs'])\n",
    "        formated_data[part]['ClassIDs'].extend([PHONE_DEF[i] for i in tmp])\n",
    "        formated_data[part]['seqStartIdx'].append(formated_data[part]['seqStartIdx'][-1]+len(tmp))\n",
    "        formated_data[part]['nTrials']+=1\n",
    "        formated_data[part]['trueSeqLengths'].append(parsed_record['nSeqElements'].numpy())\n",
    "\n",
    "    \n",
    "    for field in ['trueSeqs']:\n",
    "        formated_data[part][field] = np.array(formated_data[part][field])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "731b3d45-dc8b-422f-aa6f-a78950f618cb",
   "metadata": {},
   "source": [
    "### Data Dict `formted_data` Structure "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bc94c9a-9334-4e7a-a5d3-6d6d64813593",
   "metadata": {},
   "outputs": [],
   "source": [
    "formated_data['train'].keys()\n",
    "\n",
    "for key in formated_data['train'].keys(): \n",
    "    try: \n",
    "        print(f\"{key} : {np.shape(formated_data['train'][key])}\")\n",
    "    except:\n",
    "        print(key)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63f6b55f-7e12-44ce-982f-865641b827df",
   "metadata": {},
   "source": [
    "### Example Transcription Sentence "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "148a0736-4952-4c37-84f6-44e045a7a2af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: 324 sentences\n",
      " - december and january are nice months to spend in miami\n",
      " - vietnamese cuisine is exquisite\n",
      " - remove the splinter with a pair of tweezers\n",
      " - be careful not to plow over the flower beds\n",
      " - well now we have two big theaters\n",
      "\n",
      "\n",
      "val: 36 sentences\n",
      " - bob bandaged both wounds with the skill of a doctor\n",
      " - the diagnosis was discouraging however he was not overly worried\n",
      " - why charge money for such garbage\n",
      " - ambidextrous pickpockets accomplish more\n",
      " - gregory and tom chose to watch cartoons in the afternoon\n",
      "\n",
      "\n",
      "test: 20 sentences\n",
      " - to some extent predispositions are shaped by exposure to group environments\n",
      " - an adult male baboon's teeth are not suitable for eating shellfish\n",
      " - in this context it would do well for us to bear in mind the vision of peace\n",
      " - you're boiling milk ain't you\n",
      " - rich looked for spotted hyenas and jaguars on the safari\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for split in formated_data.keys(): \n",
    "    print(f\"{split}: {len(formated_data[split]['transcriptions'])} sentences\")\n",
    "    for i in range(5):\n",
    "        text = ''.join(chr(value) for value in formated_data[split]['transcriptions'][i] if value!=0 )\n",
    "        print(f' - {text}')\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d05f338f-12a6-428c-b076-d4d3ff243885",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "willet_dataset",
   "language": "python",
   "name": "willet_dataset"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
